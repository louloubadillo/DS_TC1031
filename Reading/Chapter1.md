# 1. The role of Algorithms in Computing
## 1.1 Algorithms 

### What is an algorithm? 
An <b>algorithm</b> is a sequence of computational steps that transform an input into an output. <br>
We can also view it as a tool for solving a specified <b>computational problem</b> <br><br>
An algorithm is said to be <b>correct</b> if, for every input instance, it halts with the correct output. <br>

## 2. Algorithms as a technology 
If computers were infinitely fast and computer memory was free, we would still want to study algorithms because we need to be able to demonstrate that our solution method terminates
and does so with the correct answer. <br><br>
If computers were infinitely fast, any correct method for solving a problem
would do. The reality is that, computers may be fast, but they are not infinitely fast. And memory
may be inexpensive, but it is not free.<br>
Computing time is therefore a bounded
resource, and so is space in memory. We need to use these resources wisely, and
algorithms that are <b>efficient</b> in terms of time or space help us do so. 

### Efficiency 
Different algorithms devised to solve the same problem often differ dramatically in their efficiency. These differences can be much more significant than differences due to hardware and software. 

### Algorithms and other technologies
We should consider algorithms, like computer hardware, as a technology. Total system performance depends on choosing efficient
algorithms as much as on choosing fast hardware. Just as rapid advances are being made in other computer technologies, they are being made in algorithms as well. <br> <br>
Algorithms are at the core of most technologies used in contemporary computers.<br>
Furthermore, with the ever-increasing capacities of computers, we use them to solve larger problems than ever before, it is at larger problem sizes that the differences
in efficiency between algorithms become particularly prominent.

